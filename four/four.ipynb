{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T07:12:01.804114Z",
     "iopub.status.busy": "2025-04-11T07:12:01.803866Z",
     "iopub.status.idle": "2025-04-11T07:12:06.840866Z",
     "shell.execute_reply": "2025-04-11T07:12:06.839935Z",
     "shell.execute_reply.started": "2025-04-11T07:12:01.804073Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T07:12:09.902308Z",
     "iopub.status.busy": "2025-04-11T07:12:09.901826Z",
     "iopub.status.idle": "2025-04-11T07:12:09.906841Z",
     "shell.execute_reply": "2025-04-11T07:12:09.906217Z",
     "shell.execute_reply.started": "2025-04-11T07:12:09.902286Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multiply by sqrt(d_model) as per the paper\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T07:13:28.967196Z",
     "iopub.status.busy": "2025-04-11T07:13:28.966908Z",
     "iopub.status.idle": "2025-04-11T07:13:28.973698Z",
     "shell.execute_reply": "2025-04-11T07:13:28.972853Z",
     "shell.execute_reply.started": "2025-04-11T07:13:28.967173Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # Shape: (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # Shape: (d_model/2)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # Apply sin to even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # Apply cos to odd indices\n",
    "\n",
    "        pe = pe.unsqueeze(0) # Shape: (1, max_len, d_model) - Add batch dimension\n",
    "\n",
    "        # Register buffer makes it part of the model's state, but not a parameter\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        # Add positional encoding to the input embeddings\n",
    "        # self.pe[:, :x.size(1)] selects encodings up to the sequence length of x\n",
    "        x = x + self.pe[:, :x.size(1)].requires_grad_(False) # Don't train positional encodings\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T07:13:41.763358Z",
     "iopub.status.busy": "2025-04-11T07:13:41.763082Z",
     "iopub.status.idle": "2025-04-11T07:13:41.768102Z",
     "shell.execute_reply": "2025-04-11T07:13:41.767338Z",
     "shell.execute_reply.started": "2025-04-11T07:13:41.763340Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        # Learnable parameters\n",
    "        self.gamma = nn.Parameter(torch.ones(features)) # scale\n",
    "        self.beta = nn.Parameter(torch.zeros(features)) # offset\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, features)\n",
    "        mean = x.mean(-1, keepdim=True) # Calculate mean over the last dimension (features)\n",
    "        std = x.std(-1, keepdim=True)   # Calculate std dev over the last dimension\n",
    "        # Normalize\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T07:13:50.567466Z",
     "iopub.status.busy": "2025-04-11T07:13:50.566909Z",
     "iopub.status.idle": "2025-04-11T07:13:50.572312Z",
     "shell.execute_reply": "2025-04-11T07:13:50.571392Z",
     "shell.execute_reply.started": "2025-04-11T07:13:50.567446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # W1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # W2\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        # FFN(x) = max(0, xW1 + b1)W2 + b2\n",
    "        return self.linear_2(self.dropout(self.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T07:14:21.900032Z",
     "iopub.status.busy": "2025-04-11T07:14:21.899427Z",
     "iopub.status.idle": "2025-04-11T07:14:21.909019Z",
     "shell.execute_reply": "2025-04-11T07:14:21.908301Z",
     "shell.execute_reply.started": "2025-04-11T07:14:21.900006Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads # Dimension of keys/queries/values per head\n",
    "\n",
    "        # Linear layers for Query, Key, Value, and final output\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention_scores = None # To store attention scores for visualization if needed\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask=None, dropout=None):\n",
    "        d_k = query.size(-1)\n",
    "        # scores shape: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "             # Apply mask (typically for padding or causal attention)\n",
    "             # Mask should be broadcastable to scores shape\n",
    "             scores = scores.masked_fill(mask == 0, -1e9) # Fill with very small value\n",
    "\n",
    "        # p_attn shape: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        p_attn = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        # output shape: (batch_size, num_heads, seq_len_q, d_k)\n",
    "        output = torch.matmul(p_attn, value)\n",
    "        return output, p_attn # Return context vector and attention weights\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query, key, value shape: (batch_size, seq_len, d_model)\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1) Apply linear projections and split into heads\n",
    "        # q, k, v shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        q = self.w_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 2) Apply attention on all heads in parallel\n",
    "        # x shape: (batch_size, num_heads, seq_len_q, d_k)\n",
    "        # self.attention_scores shape: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        x, self.attention_scores = MultiHeadAttention.attention(q, k, v, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) Concatenate heads and apply final linear layer\n",
    "        # x shape: (batch_size, seq_len_q, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        x = self.w_o(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T07:14:32.687986Z",
     "iopub.status.busy": "2025-04-11T07:14:32.687745Z",
     "iopub.status.idle": "2025-04-11T07:14:32.692522Z",
     "shell.execute_reply": "2025-04-11T07:14:32.691629Z",
     "shell.execute_reply.started": "2025-04-11T07:14:32.687969Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # Apply residual connection to the output of the sublayer\n",
    "        # The sublayer is a function (like multi-head attention or feed-forward)\n",
    "        # return x + self.dropout(sublayer(self.norm(x))) # Original paper: Norm is applied before sublayer\n",
    "        # Post-LN variation (often performs well or better): Apply norm after adding residual\n",
    "        return self.norm(x + self.dropout(sublayer(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T07:14:42.825508Z",
     "iopub.status.busy": "2025-04-11T07:14:42.825210Z",
     "iopub.status.idle": "2025-04-11T07:14:42.831502Z",
     "shell.execute_reply": "2025-04-11T07:14:42.830534Z",
     "shell.execute_reply.started": "2025-04-11T07:14:42.825487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, self_attn: MultiHeadAttention, feed_forward: PositionwiseFeedForward, dropout: float):\n",
    "        super().__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.res_connect_1 = ResidualConnection(d_model, dropout)\n",
    "        self.res_connect_2 = ResidualConnection(d_model, dropout)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        # mask shape: (batch_size, 1, 1, seq_len) or similar broadcastable shape\n",
    "\n",
    "        # Sublayer 1: Multi-Head Self-Attention + Add & Norm\n",
    "        # Pass x as query, key, and value for self-attention\n",
    "        x = self.res_connect_1(x, lambda x: self.self_attn(x, x, x, mask))\n",
    "\n",
    "        # Sublayer 2: Feed Forward + Add & Norm\n",
    "        x = self.res_connect_2(x, self.feed_forward)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T07:14:57.243662Z",
     "iopub.status.busy": "2025-04-11T07:14:57.242975Z",
     "iopub.status.idle": "2025-04-11T07:14:57.248484Z",
     "shell.execute_reply": "2025-04-11T07:14:57.247693Z",
     "shell.execute_reply.started": "2025-04-11T07:14:57.243626Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer: EncoderLayer, num_layers: int):\n",
    "        super().__init__()\n",
    "        # Create N identical layers\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(num_layers)])\n",
    "        self.norm = LayerNorm(layer.d_model) # Final normalization layer\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        # mask shape: Broadcastable to attention scores\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x) # Apply final normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T07:15:15.542120Z",
     "iopub.status.busy": "2025-04-11T07:15:15.541413Z",
     "iopub.status.idle": "2025-04-11T07:15:15.548030Z",
     "shell.execute_reply": "2025-04-11T07:15:15.547106Z",
     "shell.execute_reply.started": "2025-04-11T07:15:15.542096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, self_attn: MultiHeadAttention, cross_attn: MultiHeadAttention, feed_forward: PositionwiseFeedForward, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.self_attn = self_attn # Masked self-attention\n",
    "        self.cross_attn = cross_attn # Cross-attention (query=decoder, key/value=encoder output)\n",
    "        self.feed_forward = feed_forward\n",
    "        self.res_connect_1 = ResidualConnection(d_model, dropout)\n",
    "        self.res_connect_2 = ResidualConnection(d_model, dropout)\n",
    "        self.res_connect_3 = ResidualConnection(d_model, dropout)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        # x shape (decoder input): (batch_size, tgt_seq_len, d_model)\n",
    "        # memory shape (encoder output): (batch_size, src_seq_len, d_model)\n",
    "        # src_mask: Masks encoder padding. Shape: (batch_size, 1, 1, src_seq_len)\n",
    "        # tgt_mask: Masks decoder padding and future tokens. Shape: (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
    "\n",
    "        # Sublayer 1: Masked Multi-Head Self-Attention + Add & Norm\n",
    "        # Pass x as query, key, value; use target mask\n",
    "        x = self.res_connect_1(x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "\n",
    "        # Sublayer 2: Multi-Head Cross-Attention + Add & Norm\n",
    "        # Query comes from decoder (x), Key/Value come from encoder output (memory)\n",
    "        # Use source mask here\n",
    "        x = self.res_connect_2(x, lambda x: self.cross_attn(x, memory, memory, src_mask))\n",
    "\n",
    "        # Sublayer 3: Feed Forward + Add & Norm\n",
    "        x = self.res_connect_3(x, self.feed_forward)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T07:15:28.272984Z",
     "iopub.status.busy": "2025-04-11T07:15:28.272709Z",
     "iopub.status.idle": "2025-04-11T07:15:28.277783Z",
     "shell.execute_reply": "2025-04-11T07:15:28.277147Z",
     "shell.execute_reply.started": "2025-04-11T07:15:28.272963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer: DecoderLayer, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(num_layers)])\n",
    "        self.norm = LayerNorm(layer.d_model) # Final normalization layer\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        # x shape (decoder input): (batch_size, tgt_seq_len, d_model)\n",
    "        # memory shape (encoder output): (batch_size, src_seq_len, d_model)\n",
    "        # src_mask/tgt_mask: Appropriate masks\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x) # Apply final normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T07:15:38.412779Z",
     "iopub.status.busy": "2025-04-11T07:15:38.412172Z",
     "iopub.status.idle": "2025-04-11T07:15:38.416785Z",
     "shell.execute_reply": "2025-04-11T07:15:38.416201Z",
     "shell.execute_reply.started": "2025-04-11T07:15:38.412749Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        # Output shape: (batch_size, seq_len, vocab_size)\n",
    "        # Log softmax is often used with NLLLoss for training\n",
    "        return torch.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T07:16:05.256824Z",
     "iopub.status.busy": "2025-04-11T07:16:05.256147Z",
     "iopub.status.idle": "2025-04-11T07:16:05.262509Z",
     "shell.execute_reply": "2025-04-11T07:16:05.261827Z",
     "shell.execute_reply.started": "2025-04-11T07:16:05.256800Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: Encoder,\n",
    "                 decoder: Decoder,\n",
    "                 src_embed: InputEmbeddings,\n",
    "                 tgt_embed: InputEmbeddings,\n",
    "                 pos_enc: PositionalEncoding,\n",
    "                 projection: ProjectionLayer):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.pos_enc = pos_enc # Use the same PE for src and tgt embeddings\n",
    "        self.projection = projection\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # Embed source sequence, add positional encoding, pass through encoder\n",
    "        return self.encoder(self.pos_enc(self.src_embed(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt, memory, src_mask, tgt_mask):\n",
    "        # Embed target sequence, add positional encoding, pass through decoder\n",
    "        return self.decoder(self.pos_enc(self.tgt_embed(tgt)), memory, src_mask, tgt_mask)\n",
    "\n",
    "    def project(self, x):\n",
    "        # Project decoder output to vocabulary space\n",
    "        return self.projection(x)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        # src: (batch_size, src_seq_len)\n",
    "        # tgt: (batch_size, tgt_seq_len)\n",
    "        # src_mask: (batch_size, 1, 1, src_seq_len) - Masks padding in src\n",
    "        # tgt_mask: (batch_size, 1, tgt_seq_len, tgt_seq_len) - Masks padding and future tokens in tgt\n",
    "\n",
    "        encoder_output = self.encode(src, src_mask)\n",
    "        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n",
    "        output = self.project(decoder_output)\n",
    "        return output # Shape: (batch_size, tgt_seq_len, tgt_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T07:16:21.869541Z",
     "iopub.status.busy": "2025-04-11T07:16:21.868843Z",
     "iopub.status.idle": "2025-04-11T07:16:21.875739Z",
     "shell.execute_reply": "2025-04-11T07:16:21.874836Z",
     "shell.execute_reply.started": "2025-04-11T07:16:21.869521Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int,\n",
    "                      d_model: int = 512, num_layers: int = 6, num_heads: int = 8,\n",
    "                      d_ff: int = 2048, dropout: float = 0.1, max_len: int = 5000) -> Transformer:\n",
    "\n",
    "    # Create embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create positional encoding layer\n",
    "    pos_enc = PositionalEncoding(d_model, dropout, max_len)\n",
    "\n",
    "    # Create Multi-Head Attention, Feed Forward, and Encoder/Decoder layers\n",
    "    attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    encoder_layer = EncoderLayer(d_model, copy.deepcopy(attn), copy.deepcopy(ff), dropout)\n",
    "    decoder_layer = DecoderLayer(d_model, copy.deepcopy(attn), copy.deepcopy(attn), copy.deepcopy(ff), dropout)\n",
    "\n",
    "    # Create Encoder and Decoder stacks\n",
    "    encoder = Encoder(encoder_layer, num_layers)\n",
    "    decoder = Decoder(decoder_layer, num_layers)\n",
    "\n",
    "    # Create projection layer\n",
    "    projection = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Assemble the Transformer model\n",
    "    model = Transformer(encoder, decoder, src_embed, tgt_embed, pos_enc, projection)\n",
    "\n",
    "    # Initialize parameters (Xavier/Glorot recommended in the paper)\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T07:16:54.055370Z",
     "iopub.status.busy": "2025-04-11T07:16:54.054809Z",
     "iopub.status.idle": "2025-04-11T07:16:55.132598Z",
     "shell.execute_reply": "2025-04-11T07:16:55.131879Z",
     "shell.execute_reply.started": "2025-04-11T07:16:54.055347Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Building and Testing Transformer ---\n",
      "Model built successfully. Parameter count: 60,535,544\n",
      "\n",
      "Input shapes:\n",
      "src_tokens:        torch.Size([4, 10])\n",
      "tgt_tokens:        torch.Size([4, 12])\n",
      "src_padding_mask:  torch.Size([4, 1, 1, 10])\n",
      "tgt_combined_mask: torch.Size([4, 1, 12, 12])\n",
      "\n",
      "Output shape (log probabilities): torch.Size([4, 12, 11000])\n",
      "--- Example Completed ---\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(\"--- Building and Testing Transformer ---\")\n",
    "\n",
    "    # Define parameters\n",
    "    SRC_VOCAB_SIZE = 10000\n",
    "    TGT_VOCAB_SIZE = 11000\n",
    "    D_MODEL = 512\n",
    "    NUM_LAYERS = 6\n",
    "    NUM_HEADS = 8\n",
    "    D_FF = 2048\n",
    "    DROPOUT = 0.1\n",
    "    MAX_LEN = 100 # Max sequence length for example\n",
    "\n",
    "    # Build the model\n",
    "    transformer_model = build_transformer(\n",
    "        SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, D_MODEL, NUM_LAYERS, NUM_HEADS, D_FF, DROPOUT, MAX_LEN\n",
    "    )\n",
    "    print(f\"Model built successfully. Parameter count: {sum(p.numel() for p in transformer_model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "    # Create dummy input data\n",
    "    BATCH_SIZE = 4\n",
    "    SRC_SEQ_LEN = 10\n",
    "    TGT_SEQ_LEN = 12\n",
    "\n",
    "    # Dummy source and target sequences (indices)\n",
    "    src_tokens = torch.randint(1, SRC_VOCAB_SIZE, (BATCH_SIZE, SRC_SEQ_LEN)) # Avoid 0 for padding usually\n",
    "    tgt_tokens = torch.randint(1, TGT_VOCAB_SIZE, (BATCH_SIZE, TGT_SEQ_LEN))\n",
    "\n",
    "    # Create dummy masks\n",
    "    # Source padding mask (mask where src_tokens == 0, assuming 0 is padding)\n",
    "    src_padding_mask = (src_tokens != 0).unsqueeze(1).unsqueeze(2) # Shape: (B, 1, 1, S)\n",
    "    # Target padding mask (mask where tgt_tokens == 0)\n",
    "    tgt_padding_mask = (tgt_tokens != 0).unsqueeze(1).unsqueeze(2) # Shape: (B, 1, 1, T)\n",
    "    # Target subsequent/causal mask (prevents attending to future tokens)\n",
    "    tgt_seq_len = tgt_tokens.size(1)\n",
    "    tgt_subsequent_mask = torch.tril(torch.ones((tgt_seq_len, tgt_seq_len), dtype=torch.bool)).unsqueeze(0).unsqueeze(0) # Shape: (1, 1, T, T)\n",
    "    # Combine target padding and subsequent masks\n",
    "    tgt_combined_mask = tgt_padding_mask & tgt_subsequent_mask # Shape: (B, 1, T, T)\n",
    "\n",
    "    print(f\"\\nInput shapes:\")\n",
    "    print(f\"src_tokens:        {src_tokens.shape}\")\n",
    "    print(f\"tgt_tokens:        {tgt_tokens.shape}\")\n",
    "    print(f\"src_padding_mask:  {src_padding_mask.shape}\")\n",
    "    print(f\"tgt_combined_mask: {tgt_combined_mask.shape}\")\n",
    "\n",
    "\n",
    "    # Forward pass\n",
    "    transformer_model.eval() # Set model to evaluation mode (disables dropout)\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        output = transformer_model(src_tokens, tgt_tokens, src_padding_mask, tgt_combined_mask)\n",
    "\n",
    "    print(f\"\\nOutput shape (log probabilities): {output.shape}\") # Should be (BATCH_SIZE, TGT_SEQ_LEN, TGT_VOCAB_SIZE)\n",
    "    print(\"--- Example Completed ---\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
