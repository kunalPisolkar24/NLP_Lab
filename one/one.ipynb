{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqUeIReFpUDj",
        "outputId": "d011ba36-b460-4733-cf9e-884288df4c37"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üìå Import necessary libraries from NLTK and others\n",
        "import nltk\n",
        "import string"
      ],
      "metadata": {
        "id": "M10AeEzipk34"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK data (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUUpsq7fppYi",
        "outputId": "f6c62a8a-b406-4167-94a7-122b66ac2352"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizers from NLTK\n",
        "from nltk.tokenize import WhitespaceTokenizer, word_tokenize, TreebankWordTokenizer, TweetTokenizer, MWETokenizer\n",
        "\n",
        "# Stemmers\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "\n",
        "# Lemmatizer\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "Sk50umVIpsHT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîπ Sample text for processing\n",
        "text = \"Hello, world! This is an example sentence for tokenization. Let's see how it performs? üòä\""
      ],
      "metadata": {
        "id": "D-zB92bUpuVK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚û°Ô∏è Whitespace Tokenization\n",
        "whitespace_tok = WhitespaceTokenizer().tokenize(text)\n",
        "print(\"Whitespace Tokenization:\")\n",
        "print(whitespace_tok, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nEjR9Smpxnj",
        "outputId": "787d2f24-16cb-4528-a4d3-515cc7de9daf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace Tokenization:\n",
            "['Hello,', 'world!', 'This', 'is', 'an', 'example', 'sentence', 'for', 'tokenization.', \"Let's\", 'see', 'how', 'it', 'performs?', 'üòä'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚û°Ô∏è Punctuation-based Tokenization using word_tokenize (splits punctuation)\n",
        "punctuation_tok = word_tokenize(text)\n",
        "print(\"Punctuation-based Tokenization (word_tokenize):\")\n",
        "print(punctuation_tok, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilg9MK8Ap0LR",
        "outputId": "c6d51663-467f-4d6c-e045-83a59913c655"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation-based Tokenization (word_tokenize):\n",
            "['Hello', ',', 'world', '!', 'This', 'is', 'an', 'example', 'sentence', 'for', 'tokenization', '.', 'Let', \"'s\", 'see', 'how', 'it', 'performs', '?', 'üòä'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚û°Ô∏è Treebank Tokenizer\n",
        "treebank_tok = TreebankWordTokenizer().tokenize(text)\n",
        "print(\"Treebank Tokenization:\")\n",
        "print(treebank_tok, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIaT3K0AqKwW",
        "outputId": "d1a9a181-37bd-48f5-f320-906c594f5ee5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treebank Tokenization:\n",
            "['Hello', ',', 'world', '!', 'This', 'is', 'an', 'example', 'sentence', 'for', 'tokenization.', 'Let', \"'s\", 'see', 'how', 'it', 'performs', '?', 'üòä'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚û°Ô∏è Tweet Tokenizer (handles emojis, hashtags, etc.)\n",
        "tweet_tok = TweetTokenizer().tokenize(text)\n",
        "print(\"Tweet Tokenization:\")\n",
        "print(tweet_tok, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YG7CUr2hqaC3",
        "outputId": "e112e62f-6a8c-45ed-9bb1-71def29ae1e9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet Tokenization:\n",
            "['Hello', ',', 'world', '!', 'This', 'is', 'an', 'example', 'sentence', 'for', 'tokenization', '.', \"Let's\", 'see', 'how', 'it', 'performs', '?', 'üòä'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚û°Ô∏è Multi-Word Expression (MWE) Tokenizer\n",
        "# For demonstration, we define an MWE for \"New York\" (if it existed in text)\n",
        "mwe_tok = MWETokenizer([(\"New\", \"York\")])\n",
        "# Tokenize first using word_tokenize then apply MWETokenizer\n",
        "mwe_tokens = mwe_tok.tokenize(word_tokenize(text))\n",
        "print(\"MWE Tokenization (for phrases like 'New York'):\")\n",
        "print(mwe_tokens, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAzMK2Lkqc5H",
        "outputId": "259636d9-fcfa-4dca-b80f-7023c2f1a308"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MWE Tokenization (for phrases like 'New York'):\n",
            "['Hello', ',', 'world', '!', 'This', 'is', 'an', 'example', 'sentence', 'for', 'tokenization', '.', 'Let', \"'s\", 'see', 'how', 'it', 'performs', '?', 'üòä'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚û°Ô∏è Using Porter Stemmer\n",
        "porter = PorterStemmer()\n",
        "porter_stems = [porter.stem(word) for word in punctuation_tok]\n",
        "print(\"Porter Stemmer:\")\n",
        "print(porter_stems, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AJCsjQ2qmSk",
        "outputId": "1bc38c95-1259-4e0f-bae5-df56bdce4143"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer:\n",
            "['hello', ',', 'world', '!', 'thi', 'is', 'an', 'exampl', 'sentenc', 'for', 'token', '.', 'let', \"'s\", 'see', 'how', 'it', 'perform', '?', 'üòä'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚û°Ô∏è Using Snowball Stemmer (for English)\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "snowball_stems = [snowball.stem(word) for word in punctuation_tok]\n",
        "print(\"Snowball Stemmer:\")\n",
        "print(snowball_stems, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGWMmYLXqnxA",
        "outputId": "1d1176d6-98d3-45dd-ca89-349284277952"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Snowball Stemmer:\n",
            "['hello', ',', 'world', '!', 'this', 'is', 'an', 'exampl', 'sentenc', 'for', 'token', '.', 'let', \"'s\", 'see', 'how', 'it', 'perform', '?', 'üòä'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚û°Ô∏è Using WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# For lemmatization, it is usually more effective if you provide POS tags.\n",
        "# For simplicity, we're using the default which assumes nouns.\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in punctuation_tok]\n",
        "print(\"Lemmatization (default as nouns):\")\n",
        "print(lemmatized_tokens, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wIWxnZyqE1p",
        "outputId": "21300e86-ea64-4265-dba2-1d5392d624ab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization (default as nouns):\n",
            "['Hello', ',', 'world', '!', 'This', 'is', 'an', 'example', 'sentence', 'for', 'tokenization', '.', 'Let', \"'s\", 'see', 'how', 'it', 'performs', '?', 'üòä'] \n",
            "\n"
          ]
        }
      ]
    }
  ]
}